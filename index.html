<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
	<head>
	  <meta charset="utf-8" />
	  <title>Natural Language Interface Accessibility User Requirements</title>
	  		<script src="https://www.w3.org/Tools/respec/respec-w3c" class="remove" defer="defer"></script>
		<script src="respec-config.js" class="remove"></script>
		<script src="../biblio.js" class="remove" defer="defer"></script>
	</head>
	<body>
	  <section id="abstract">
	    <p>This document outlines various accessibility-related user needs, requirements and scenarios for natural language interfaces. These user needs should influence accessibility requirements in various related specifications, as well as the design of applications that include natural language interfaces including speech and voice input. The concept of a natural language interface is first clarified. User needs and associated requirements are then described.</p>
	    <p>This document is most explicitly not a collection of baseline requirements. It is also important to note that some of the requirements may be implemented at a system or platform level, and some may be authoring requirements for development of applications.</p>
	  </section>
	  <section id="sotd">
	  </section>
	  <section>
	    <h2>Introduction</h2>
	    <section>
	      <h3>What is a Natural Language Interface?</h3>
	      <p>A <dfn>natural language interface</dfn> is a user interface in which the user and the system communicate via a natural (human) language. The user provides input as sentences via speech or some other input, and the system generates responses in the form of sentences delivered by speech, text or another suitable modality). </p>

	  
	      <p>Often, systems that provide natural language interfaces support spoken interaction. In this scenario, speech recognition is used to process the user's input, and speech synthesis is used to generate the system's spoken responses. However, the use of speech is not essential to a natural language interface.</p>
	      <p>Typical examples of natural language interfaces include:</p>
	      <ul>
		<li><strong>Voice agents designed primarily to communicate via speech</strong>. These agents may run on a computing device such as a mobile phone, tablet, laptop or desktop computer. They may also be embedded in specialized hardware such as consumer appliances, or the automation system of a vehicle.</li>
		<li><strong>Chat bots included in Web applications</strong> that process natural language requests from the user. For example, a customer service application available on an organization's web site could offer a natural language user interface to handle customers' inquiries.</li>
		<li><dfn>Interactive Voice Response</dfn> (<abbr title="Interactive Voice Response">IVR</abbr>) systems that interact with the user via a telephone call, accepting speech or key pad input and generating speech output.</li>
	      </ul>
	      <p>These examples are indicative of applications that use natural language interfaces. They are not definitive. Variations of the examples and applications that do not fit these patterns at all are possible.</p>
	    </section>



	    <section>
	      <h3>Natural Language Interfaces and Accessibility</h3>
	      <p>The accessibility of natural language interfaces to users with disabilities can be supported by a variety of features at the platform and application levels, including assistive technologies. Multiple modes of input and output should be provided to enable interaction with the system by users who have a variety of physical and sensory capabilities. For example, whereas speech input may be needed by some users with physical disabilities, keyboard input, switch input, or an eye tracking system may be needed by other users. </p>
	      <p>Similarly, natural language output may be spoken, or it may be provided as visually displayed text. These and other requirements are elaborated further below. In some cases, these requirements may best be satisfied by an assistive technology. For example, a chat bot that does not provide a spoken interface directly may nevertheless satisfy a user's need for speech input via a dictation function provided as part of the user's browser or operating system. There may also be other 'service' aspects that are specifically needed to better support accessibility that operate at a different layer to any modality input or output considerations. </p>
	      <p>The design of the application should support the cognitive needs of users, including those who have learning or cognitive disabilities. Discoverability, simplicity, and affordances for example, are important considerations in the design of the natural language interaction.</p>
	    </section>

	        <section>
	    	<h3>Voice User Interfaces</h3>
	    	<p>Voice user interfaces (VUI) using speech such as those found on a range of commercially available devices for home and mobile use represent a part of the stack that make up natural language interfaces. This document aims to identify accessibility related user needs and requirements for VUIs and indicate further areas of work and research in terms of how they relate to new standards like WCAG 3 and other emerging technologies.</p>
	    </section>
	    <section>
	      <h3>Scope</h3>
	      <p>Natural language interfaces frequently occur as components of larger user interfaces and systems. For example, a chat bot may be included in a web application. A natural language interface may be an essential part of a multi-modal application that uses a combinations of language and gestural inputs. An example would be an interactive navigation tool that allows the user to issue spoken commands and to interact with a graphical map with a pointing device.</p>
	      <p>The scope of this document is largely confined to the accessibility of the natural language aspect of the over-all user interface. It is concerned with the accessibility of natural language interactions to users with disabilities.</p>


	      <p class="ednote">
	      The scope of this work is currently under active  discussion in the Research Questions Task Force.</p>
		</section>
	      <section>

	       <h3>Services and Agents</h3>
	       <p>Behind these interfaces there are services that provide core processing, evaluation and content. This document aims to look at these services and determine to what degree they can and should support the needs of people with disabilities; what system requirements are, or where further research is needed. </p>
	             <p>Ideally by satisfying system requirements, developers of platforms and applications offering natural language interfaces can meet corresponding user needs. Currently, no stance is taken in this document regarding which needs are best satisfied at the platform level, by an assistive technology, or in the development of applications, but this will change as the document develops. These architectural considerations are left to be decided by system designers, and therefore there may be requirements in accessible system design that they need to be aware of. Often, they also depend on the services provided by the underlying operating system or by the web platform.</p>

	      <p>If natural language interaction is provided as part of a system that also offers other styles of interaction, this document should be read in combination with guidance provided elsewhere which is relevant to the other interface and service aspects. Notably,</p>
	      <ul>
		<li><a href="https://www.w3.org/TR/WCAG21/">Web Content Accessibility Guidelines (<abbr title="Web content Accessibility Guidelines">WCAG</abbr>) 2.1</a> [[WCAG21]] establishes normative requirements for the accessibility of web-based applications.</li>
		<li><a href="https://www.w3.org/TR/raur/"><abbr title="Real-Time Communication">RTC</abbr> Accessibility User Requirements (<abbr>RAUR</abbr>)</a> [[raur]] identifies user needs and corresponding requirements for the accessibility of real-time communication applications, such as video conference tools and web-based telephony systems.</li>
		<li><a href="https://www.w3.org/TR/xaur/">XR Accessibility User Requirements (<abbr>XAUR</abbr>)</a> [[xaur]] identifies user needs and corresponding requirements for the accessibility of virtual reality and augmented reality.</li>
		<li><a href="https://www.w3.org/WAI/standards-guidelines/uaag/">User Agent Accessibility Guidelines (<abbr>UAAG</abbr>)</a> [[uaag]]- if we consider the service behind the interface - what parts of the User Agent Accessibility Guidelines (UAAG) are relevant for these particular services?</li>
	      </ul>
	      <p>As a general principle, the entire interface of a system or application needs to be accessible to users with disabilities. If only the natural language interaction component is accessible, some users will be unable to complete tasks successfully. For example, a smart agent that answers a user's questions by searching the web for information and then displaying it on screen is only accessible as a whole if both the  interaction and the presentation of the information satisfy the user's access needs. If the on-screen information is not accessible, then the user cannot complete the task of acquiring and understanding the information requested.</p>

	    </section>
	    <section>
	      <h3>User need definition</h3>
	      <p><dfn>User needs</dfn> relate to what a user needs from a particular application or platform to complete a task or to achieve a particular goal. User needs are dependent on the context in which an application is used, including the user's capabilities and the environmental conditions in which interaction with the interface takes place. For example, a spoken interaction would be inaccessible to a person who is deaf, or to a hearing person situated in a noisy environment. Although disability-related needs are the focus of this document, the user needs described here are not limited to people with specific types of disability. The capabilities of users vary greatly. They include a variety of physical, sensory, learning and cognitive abilities that should be taken into account in the design of platforms and applications.</p>

	      <p>The user needs and associated requirements are actively being reviewed by the Research Questions Task Force (<abbr title="Research Questions Task Force">RQTF</abbr>) and by the Accessible Platform Architectures (<abbr title="Accessible Platform Architectures">APA</abbr>) Working Group.</p>
	    </section>
	  </section>
	  <section>
	    <h2>User needs and requirements</h2>
	    <p>This section outlines a variety of user needs and system requirements that can satisfy them.</p>
	    <section>
	      <h3>User Identification and Authentication</h3>
	      <ul>
		<li><strong>User Need 1:</strong> A user with a physical disability needs to use speech as the only means of communicating with a system that can be shared with other users. Due to security and privacy requirements, each user must be authenticated individually.</li>
		<li><strong>REQ 1:</strong> Support voice identification as a means of biometric authentication.</li>
	      </ul>
	      <p class="note">
		To achieve adequate security, voice identification may need to be combined with other factors of authentication.</p>

		<ul>
		  <li><strong>User Need 2:</strong> A user who is deaf or who has a speech disability needs to interact with a system that can be shared with other users. Due to security and privacy requirements, each user must be authenticated individually.</li>
		  <li><strong>REQ 2a:</strong>Support a means of biometric authentication other than voice recognition.</li>
		  <li><strong>REQ 2b:</strong> Support a non-biometric means of authentication, such as a hardware security token.</li>
		</ul>
		<p class="note">
		In some cases, this requirement can be met simply by using authentication mechanisms provided by the underlying operating system or browser environment.</p>

		<aside class="example">
		  <p>A smart agent is designed to run under several operating systems used by mobile devices. It can use authentication methods provided by each of these platforms to identify the user reliably. These mechanisms include face recognition and fingerprint recognition.</p>
		</aside>
		<aside class="example">
		  <p>A smart agent is embedded in a public kiosk, located for example at an airport. A user can choose face recognition or voice recognition as the means of biometric authentication.</p>
		</aside>
	    </section>
	    <section>
	      <h3>Means of Input and Output</h3>
	      <ul>
		<li><strong>User Need 3:</strong> Different users have a need for different input devices or mechanisms. For example, a person with a physical disability may need speech input, or single switch input, or eye tracking input. A person who is deaf or who has a speech disability may need to use keyboard input.</li>
		<li><strong>REQ 3:</strong>Support multiple input devices and methods.</li>
	      </ul>
	      <p class="note">
		This requirement can often be met by supporting the input methods available from the underlying platform, including assistive technologies.
	      </p>
	      <p class="note">
	      If software that incorporates a natural language interface supports multiple input mechanisms, support for any specific mechanism may be available only on particular hardware devices or in particular environments. For example, a smart speaker may support only speech input, whereas the same smart agent running on a mobile system such as a phone or tablet may support text input via a keyboard or any device capable of emulating a keyboard.</p>
	      <p class="note">
	      See the requirement to support a keyboard interface specified in WCAG 2.1 [[WCAG21]], success criterion 2.1.1.</p>

	      <ul>
		<li><strong>User Need 4:</strong> Different users have a need for different output devices or mechanisms. For example, a user who is blind may require speech output. A user who is deaf or who has a speech disability may require visually displayed text output. A user who is deaf-blind may require braille output.</li>
		<li><strong>REQ 4:</strong> Support multiple output devices and methods.</li>
	      </ul>
		<p class="note">
		This requirement can often be met by supporting the output methods available from the underlying platform, including assistive technologies.</p>
		<p class="note">
		If software that incorporates a natural language interface supports multiple output mechanisms, support for any specific mechanism may be available only on particular hardware devices or in particular environments. For example, a smart speaker supports only speech output, whereas the same smart agent running on a mobile system such as a phone or tablet may support a visual display as well, and may be compatible with braille devices.</p>


		<ul>
		  <li><strong>User Need 5:</strong> A user who is deaf or hard of hearing needs to provide speech input to an application, while having the output displayed visually as text (with or without spoken output).</li>
		  <li><strong>REQ 5a:</strong>Support a mode of operation in which the user can speak to the system, and the system's natural language output is directed to a screen.</li>
		  <li><strong>REQ 5b:</strong> Support a mode of operation in which the user can speak to the system, and the system's natural language output is both spoken and directed to a screen.</li>
		</ul>
		  <ul>
		  <li><strong>User Need 6:</strong> A user with a speech disability needs to provide textual input to the system, and to receive spoken output.</li>
		  <li><strong>REQ 6:</strong> Provide a mode of operation in which keyboard or other forms of textual input can be given, in combination with speech output.</li>
		</ul>

		<ul>
		  <li><strong>User Need 7:</strong> A user who is deaf-blind needs to communicate with the system via a refreshable braille device.</li>
		  <li><strong>REQ 7:</strong> Support a mode of operation in which input and output are both provided as text.</li>
		</ul>
		<p class="note">
		  Support for braille displays is assumed to be provided by a screen reader running under the device's operating system. Therefore, support for keyboard input and textual output is the stated requirement for the natural language interface itself, leaving interaction with the braille hardware to the operating system on which the user interface is run.</p>
		</section>
		<section>
		  <h3>Communicating in a Language that the User Needs</h3>
		  <ul>
		    <li><strong>User Need 8:</strong> A user who is deaf or hard of hearing needs to communicate with the system in a sign language.</li>
		    <li><strong>REQ 8a:</strong> Provide a mode of operation in which sign language presented by the user is recognized and processed by the system.</li>
		    <li><strong>REQ 8b:</strong> Provide a mode of operation in which the system's output is presented visually in a sign language.</li>
		    <li><strong>REQ 8c:</strong> As an alternative to requirements 8a and 8b, provide a mode of operation in which a human sign language interpreter relays communication between the user and the system.</li>
		  </ul>
		  <p class="note">
		  Sign languages vary by country and region. Therefore, multiple sign languages may need to be supported, depending on the intended audience of the system.</p>

		  <ul>
		    <li><strong>User Need 9:</strong> A user with a learning or cognitive disability needs to communicate with the system in a symbol set supported by a particular augmentative and alternative communication (<abbr title="augmentative and alternative communication">AAC</abbr>) assistive technology.</li>
		  </ul>
	
		<section>
		  <h3>Speech Recognition and Speech Production</h3>
		  <ul>
		    <li><strong>User Need 10:</strong> A user with atypical speech characteristics needs to provide spoken input to the system.</li>
		    <li><strong>REQ 10a:</strong> Ensure that the system can recognize atypical varieties of speech with adequate accuracy to enable the application to be successfully used.</li>
		    <li><strong>REQ 10b:</strong> Provide a mode of operation in which the system is trained to recognize a particular user's speech more accurately than it can without training.</li>
		  </ul>

		  <ul>
		    <li><strong>User Need 11:</strong> A user with atypical speech characteristics needs opportunities to correct the system's speech recognition errors.</li>
		    <li><strong>REQ 11a:</strong> Enable the system to estimate the probability that a user's utterance has been recognized correctly.</li>
		    <li><strong>REQ 11b:</strong> If the system's confidence in its recognition of the user's utterance is below a reasonable threshold, prompt the user to repeat or confirm the request made or the information spoken.</li>
		    <li><strong>REQ 11c:</strong> Allow the user to decide at any time to interact with the system via means of input other than speech, even if a spoken dialogue is already in progress.</li>
		  </ul>
		    
		  <ul>
		    <li><strong>User Need 12:</strong> A user needs to adjust the speaking rate, volume or pitch of speech generated by the system in order to understand it well or to interact more efficiently.</li>
		    <li><strong>REQ 12:</strong> Provide an accessible user interface with which the speaking rate, pitch and volume of speech generated by the system can be configured.</li>
		  </ul>
		  <p class="note">
		  To ensure this user interface is accessible, it should satisfy relevant accessibility requirements drawn from this document or elsewhere. For example, a system could provide spoken commands, and a settings dialogue in a graphical user interface, as alternative mechanisms for configuring speech properties.</p>
		</section>
		<section>
		  <h3>Visually displayed text</h3>
		  <ul>
		    <li><strong>User Need 13:</strong> A user who has low vision or a learning disability needs to adjust the font style or spacing of text displayed by the system.</li>
		    <li><strong>REQ 13:</strong> Ensure that font properties and text spacing are configurable by the user, including font size, font style, character, word, line and paragraph spacing.</li>
		  </ul>
		  <p class="note">
		  In some cases, this requirement can be met by capabilities of the operating system or browsing environment.</p>
		  <p class="note">
		  See the text spacing requirement specified in WCAG 2.1 [[WCAG21]], success criterion 1.4.12.</p>
		</section>
	  </section>
	</body>
      </html>
      
